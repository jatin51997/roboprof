Artificia l Intelligence:
Optimization in Deep Learning
Some Slides from:  Goodfellow et al., Deep Learning
1Today
1.Feedforward in Deep Learning
2.Backpropagation in Deep Learning
•Gradient Descent
•Stochastic Gradient Decent (SGD)
•Momentum SGD
•RMSProp
•ADAM
3.Scheduled Learning
4.Hyper -Parameter (HP) Tuning
2History of AI
3
Feed -Forward in Deep Learning
4EncoderFC
Learnable Params
(FC)Learnable Params
(CONV)
𝑐1
𝑐2
.
.
𝑐𝑀Feed -Forward in Deep Learning
5𝜽𝑭𝑪N    –Number of images for training
M    –Number of Class Images
c     –Confidence prediction score
𝜽–Learnable parameter
𝒙–Input Image
𝒚–Prediction label
𝒚𝑮𝑻–Ground -truth label 𝑦=𝑐1
𝑐2
.
.
𝑐𝑀,𝑒.𝑔.𝑦𝑐𝑎𝑡=0.11
0.78
0.06
.
0.23,𝑦𝑐𝑎𝑡𝐺𝑇=0
1
0
.
0
𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙𝟏𝒙𝟐𝒙𝟑
𝒙𝑵Feed -Forward in Deep Learning
6𝜽𝑭𝑪N    –N
𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴
𝑭𝟏∈ℝ𝑯
𝟐×𝑾
𝟐×𝟑𝟐Feed -Forward in Deep Learning
7𝜽𝑭𝑪N    –N
𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴
𝑭𝟐∈ℝ𝑯
𝟒×𝑾
𝟒×𝟔𝟒Feed -Forward in Deep Learning
8𝜽𝑭𝑪N    –N
𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴
𝑭𝟑∈ℝ𝑯
𝟖×𝑾
𝟖×𝟏𝟐𝟖Feed -Forward in Deep Learning
9𝜽𝑭𝑪N    –N
𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴
𝑭𝑳∈ℝ𝟏×𝟏×𝟓𝟏𝟐Feed -Forward in Deep Learning
10𝜽𝑭𝑪
The output prediction label is generated by a function ‘f’ applied on input 
image ‘x’ processed by learnable parameters𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴𝜽=𝜽𝟏,𝜽𝟐,⋯,𝜽𝑳,𝜽𝑭𝑪𝒇
𝒚=𝒇𝒙;𝜽Back-Propagation in Deep Learning
11𝜽𝑭𝑪
•For each input image 𝒙𝒊there is a corresponding ground -truth label 𝒚𝑮𝑻
which should be matched with output prediction label 𝒚𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴𝜽=𝜽𝟏,𝜽𝟐,⋯,𝜽𝑳,𝜽𝑭𝑪𝒇
𝝐=𝑳𝒚,𝒚𝑮𝑻
𝑳: Loss -Function
Ideally Speaking: 𝝐→𝟎Back-Propagation in Deep Learning
12𝜽𝑭𝑪
•Calculate the gradient of loss prediction in terms of prediction label𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴𝜽=𝜽𝟏,𝜽𝟐,⋯,𝜽𝑳,𝜽𝑭𝑪𝒇
𝝏𝝐
𝝏𝒚=𝝏𝑳𝒚,𝒚𝑮𝑻
𝝏𝒚Back-Propagation in Deep Learning
13𝜽𝑭𝑪
•Calculate the gradient of loss prediction in terms of prediction label𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴𝜽=𝜽𝟏,𝜽𝟐,⋯,𝜽𝑳,𝜽𝑭𝑪𝒇
𝝏𝝐
𝝏𝒚=𝝏𝑳𝒚,𝒚𝑮𝑻
𝝏𝒚𝝏𝝐
𝝏𝒚Back-Propagation in Deep Learning
14𝜽𝑭𝑪
•Back-propagate the gradient of loss -function into inner layers to calculate 
the gradient of loss -function with respect to the learnable parameter of 
that particular layer𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴𝜽=𝜽𝟏,𝜽𝟐,⋯,𝜽𝑳,𝜽𝑭𝑪𝒇
𝝏𝝐
𝝏𝑭𝑳=𝝏𝝐
𝝏𝒚∙𝝏𝒚
𝝏𝑭𝑳𝝏𝝐
𝝏𝒚𝝏𝝐
𝝏𝑭𝑳𝑭𝑳Back-Propagation in Deep Learning
15𝜽𝑭𝑪
•Back-propagate the gradient of loss -function into inner layers to calculate 
the gradient of loss -function with respect to the learnable parameter of 
that particular layer
•We can now update parameter weights using gradient -descent method𝜽𝟏 𝜽𝟐 𝜽𝟑 𝜽𝟒 𝜽𝑳𝒙∈ℝ𝑯×𝑾×𝟑 𝒚∈ℝ𝑴𝜽=𝜽𝟏,𝜽𝟐,⋯,𝜽𝑳,𝜽𝑭𝑪𝒇
𝝏𝝐
𝝏𝑭𝑳=𝝏𝝐
𝝏𝒚∙𝝏𝒚
𝝏𝑭𝑳𝝏𝝐
𝝏𝒚𝝏𝝐
𝝏𝑭𝑳𝑭𝑳
𝜽𝑳𝒌+𝟏←𝜽𝑳𝒌−𝜶∙𝝏𝝐
𝝏𝜽𝑳
Updated 
ParameterPrevious 
ParameterError -
GradientLearning 
RateBack-Propagation in Deep Learning
16•Updating on a single image sample introduces noisy gradient 
direction and we can easily get stuck at local minima
•Select a mini -batch samples (from randomly shuffled data) and 
average the gradients for updating
•aka we update not for every image but batch -of-images
•Superimpose all batch gradients to step into average direction{𝒙𝟏,𝒙𝟐,⋯,𝒙𝑩}
𝜽𝒍𝒌+𝟏←𝜽𝒍𝒌+𝟏−𝝆∙𝟏
𝑩෍
𝒊=𝟏𝑩𝝏𝝐
𝝏𝑭𝒊𝒍
Updated 
ParameterPrevious 
ParameterStochastic 
Error -GradientLearning 
Rate{𝝏𝝐
𝝏𝑭𝟏𝒍,𝝏𝝐
𝝏𝑭𝟐𝒍,⋯,𝝏𝝐
𝝏𝑭𝑩𝒍}Stochastic Gradient Descent (SGD)
17
SGD with Momentum
18•Learning with SGD can be sometimes slow
•Momentum approach we can be used to accelerate learning in the 
face of exploring local minima of loss function
•High curvature
•Small but consistent gradient
•Noisy gradient
•Momentum approach accumulates an exponentially decaying moving 
average of past gradients and continues to move in their direction
The contour lines depicts a quadratic loss 
function with poor Hessian Matrix. The red 
path cutting across the contour indicates 
the path followed by momentum learning 
rule to minimize the loss function SGD with Momentum
19How to formulate it?
•Introduce a hyper -parameter (i.e. momentum) 𝛼∈[0,1)
•𝛼determines how quickly the contributions of previous gradients 
exponentially decay
•The update rule is given by
Momentum -SGD (MSGD)
20
Today
1.Feedforward in Deep Learning
2.Backpropagation in Deep Learning
•Gradient Descent
•Stochastic Gradient Decent (SGD)
•Momentum SGD
•RMSProp
•ADAM
3.Scheduled Learning
4.Hyper -Parameter (HP) Tuning
21
Where to read from rest of topics?
22Please refer to Reading Material as well as Class Discussions for the 
rest of topics.
Artificia l Intelligence:
Optimization in Deep Learning
Some Slides from:  Goodfellow et al., Deep Learning
1Today
1.Feedforward in Deep Learning
2.Backpropagation in Deep Learning
â€¢Gradient Descent
â€¢Stochastic Gradient Decent (SGD)
â€¢Momentum SGD
â€¢RMSProp
â€¢ADAM
3.Scheduled Learning
4.Hyper -Parameter (HP) Tuning
2History of AI
3
Feed -Forward in Deep Learning
4EncoderFC
Learnable Params
(FC)Learnable Params
(CONV)
ğ‘1
ğ‘2
.
.
ğ‘ğ‘€Feed -Forward in Deep Learning
5ğœ½ğ‘­ğ‘ªN    â€“Number of images for training
M    â€“Number of Class Images
c     â€“Confidence prediction score
ğœ½â€“Learnable parameter
ğ’™â€“Input Image
ğ’šâ€“Prediction label
ğ’šğ‘®ğ‘»â€“Ground -truth label ğ‘¦=ğ‘1
ğ‘2
.
.
ğ‘ğ‘€,ğ‘’.ğ‘”.ğ‘¦ğ‘ğ‘ğ‘¡=0.11
0.78
0.06
.
0.23,ğ‘¦ğ‘ğ‘ğ‘¡ğºğ‘‡=0
1
0
.
0
ğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™ğŸğ’™ğŸğ’™ğŸ‘
ğ’™ğ‘µFeed -Forward in Deep Learning
6ğœ½ğ‘­ğ‘ªN    â€“N
ğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´
ğ‘­ğŸâˆˆâ„ğ‘¯
ğŸÃ—ğ‘¾
ğŸÃ—ğŸ‘ğŸFeed -Forward in Deep Learning
7ğœ½ğ‘­ğ‘ªN    â€“N
ğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´
ğ‘­ğŸâˆˆâ„ğ‘¯
ğŸ’Ã—ğ‘¾
ğŸ’Ã—ğŸ”ğŸ’Feed -Forward in Deep Learning
8ğœ½ğ‘­ğ‘ªN    â€“N
ğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´
ğ‘­ğŸ‘âˆˆâ„ğ‘¯
ğŸ–Ã—ğ‘¾
ğŸ–Ã—ğŸğŸğŸ–Feed -Forward in Deep Learning
9ğœ½ğ‘­ğ‘ªN    â€“N
ğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´
ğ‘­ğ‘³âˆˆâ„ğŸÃ—ğŸÃ—ğŸ“ğŸğŸFeed -Forward in Deep Learning
10ğœ½ğ‘­ğ‘ª
The output prediction label is generated by a function â€˜fâ€™ applied on input 
image â€˜xâ€™ processed by learnable parametersğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´ğœ½=ğœ½ğŸ,ğœ½ğŸ,â‹¯,ğœ½ğ‘³,ğœ½ğ‘­ğ‘ªğ’‡
ğ’š=ğ’‡ğ’™;ğœ½Back-Propagation in Deep Learning
11ğœ½ğ‘­ğ‘ª
â€¢For each input image ğ’™ğ’Šthere is a corresponding ground -truth label ğ’šğ‘®ğ‘»
which should be matched with output prediction label ğ’šğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´ğœ½=ğœ½ğŸ,ğœ½ğŸ,â‹¯,ğœ½ğ‘³,ğœ½ğ‘­ğ‘ªğ’‡
ğ=ğ‘³ğ’š,ğ’šğ‘®ğ‘»
ğ‘³: Loss -Function
Ideally Speaking: ğâ†’ğŸBack-Propagation in Deep Learning
12ğœ½ğ‘­ğ‘ª
â€¢Calculate the gradient of loss prediction in terms of prediction labelğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´ğœ½=ğœ½ğŸ,ğœ½ğŸ,â‹¯,ğœ½ğ‘³,ğœ½ğ‘­ğ‘ªğ’‡
ğğ
ğğ’š=ğğ‘³ğ’š,ğ’šğ‘®ğ‘»
ğğ’šBack-Propagation in Deep Learning
13ğœ½ğ‘­ğ‘ª
â€¢Calculate the gradient of loss prediction in terms of prediction labelğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´ğœ½=ğœ½ğŸ,ğœ½ğŸ,â‹¯,ğœ½ğ‘³,ğœ½ğ‘­ğ‘ªğ’‡
ğğ
ğğ’š=ğğ‘³ğ’š,ğ’šğ‘®ğ‘»
ğğ’šğğ
ğğ’šBack-Propagation in Deep Learning
14ğœ½ğ‘­ğ‘ª
â€¢Back-propagate the gradient of loss -function into inner layers to calculate 
the gradient of loss -function with respect to the learnable parameter of 
that particular layerğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´ğœ½=ğœ½ğŸ,ğœ½ğŸ,â‹¯,ğœ½ğ‘³,ğœ½ğ‘­ğ‘ªğ’‡
ğğ
ğğ‘­ğ‘³=ğğ
ğğ’šâˆ™ğğ’š
ğğ‘­ğ‘³ğğ
ğğ’šğğ
ğğ‘­ğ‘³ğ‘­ğ‘³Back-Propagation in Deep Learning
15ğœ½ğ‘­ğ‘ª
â€¢Back-propagate the gradient of loss -function into inner layers to calculate 
the gradient of loss -function with respect to the learnable parameter of 
that particular layer
â€¢We can now update parameter weights using gradient -descent methodğœ½ğŸ ğœ½ğŸ ğœ½ğŸ‘ ğœ½ğŸ’ ğœ½ğ‘³ğ’™âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğŸ‘ ğ’šâˆˆâ„ğ‘´ğœ½=ğœ½ğŸ,ğœ½ğŸ,â‹¯,ğœ½ğ‘³,ğœ½ğ‘­ğ‘ªğ’‡
ğğ
ğğ‘­ğ‘³=ğğ
ğğ’šâˆ™ğğ’š
ğğ‘­ğ‘³ğğ
ğğ’šğğ
ğğ‘­ğ‘³ğ‘­ğ‘³
ğœ½ğ‘³ğ’Œ+ğŸâ†ğœ½ğ‘³ğ’Œâˆ’ğœ¶âˆ™ğğ
ğğœ½ğ‘³
Updated 
ParameterPrevious 
ParameterError -
GradientLearning 
RateBack-Propagation in Deep Learning
16â€¢Updating on a single image sample introduces noisy gradient 
direction and we can easily get stuck at local minima
â€¢Select a mini -batch samples (from randomly shuffled data) and 
average the gradients for updating
â€¢aka we update not for every image but batch -of-images
â€¢Superimpose all batch gradients to step into average direction{ğ’™ğŸ,ğ’™ğŸ,â‹¯,ğ’™ğ‘©}
ğœ½ğ’ğ’Œ+ğŸâ†ğœ½ğ’ğ’Œ+ğŸâˆ’ğ†âˆ™ğŸ
ğ‘©à·
ğ’Š=ğŸğ‘©ğğ
ğğ‘­ğ’Šğ’
Updated 
ParameterPrevious 
ParameterStochastic 
Error -GradientLearning 
Rate{ğğ
ğğ‘­ğŸğ’,ğğ
ğğ‘­ğŸğ’,â‹¯,ğğ
ğğ‘­ğ‘©ğ’}Stochastic Gradient Descent (SGD)
17
SGD with Momentum
18â€¢Learning with SGD can be sometimes slow
â€¢Momentum approach we can be used to accelerate learning in the 
face of exploring local minima of loss function
â€¢High curvature
â€¢Small but consistent gradient
â€¢Noisy gradient
â€¢Momentum approach accumulates an exponentially decaying moving 
average of past gradients and continues to move in their direction
The contour lines depicts a quadratic loss 
function with poor Hessian Matrix. The red 
path cutting across the contour indicates 
the path followed by momentum learning 
rule to minimize the loss function SGD with Momentum
19How to formulate it?
â€¢Introduce a hyper -parameter (i.e. momentum) ğ›¼âˆˆ[0,1)
â€¢ğ›¼determines how quickly the contributions of previous gradients 
exponentially decay
â€¢The update rule is given by
Momentum -SGD (MSGD)
20
Today
1.Feedforward in Deep Learning
2.Backpropagation in Deep Learning
â€¢Gradient Descent
â€¢Stochastic Gradient Decent (SGD)
â€¢Momentum SGD
â€¢RMSProp
â€¢ADAM
3.Scheduled Learning
4.Hyper -Parameter (HP) Tuning
21
Where to read from rest of topics?
22Please refer to Reading Material as well as Class Discussions for the 
rest of topics.
1Artificial Intelligence: 
Introduction to Neural Networks
Perceptron, Backpropagation2Today
â—¼Neural Networks 
â‘Perceptrons
â‘Backpropagation
https://www.linkedin.com/pulse/goedels -incompleteness -theorem -emergence -ai-eberhard -schoeneburg/
3Neural Networks
â—¼Radically different approach to reasoning and 
learning
â—¼Inspired by biology
â‘the neurons in the human brain
â—¼Set of many simple processing units (neurons) 
connected together
â—¼Behavior of each neuron is very simple
â‘but a collection of neurons can have sophisticated 
behavior and can be used for complex tasks 
â—¼In a neural network, the behavior depends on 
weights on the connection between the neurons
â—¼The weights will be learned given training data4Biological Neurons
â—¼Human brain = 
â‘100 billion neurons
â‘each neuron may be connected to 
10,000 other neurons
â‘passing signals to each other via 
1,000 trillion synapses 
â—¼A neuron is made of:
â‘Dendrites : filaments that 
provide input to the neuron
â‘Axon : sends an output signal
â‘Synapses : connection with other 
neurons â€“releases 
neurotransmitters to other 
neurons
Source: http://www.human -memory.net/brain_neurons.html5Behavior of a Neuron
â—¼A neuron receives inputs from its neighbors
â—¼If enough inputs are received at the same time:
â‘the neuron is activated
â‘and fires an output to its neighbors
â—¼Repeated firings across a synapse increases its 
sensitivity and the future likelihood of its firing
â—¼If a particular stimulus repeatedly causes activity in a 
group of neurons, they become strongly associated6Today
â—¼Neural Networks 
â‘Perceptrons
â‘Backpropagation
https://www.linkedin.com/pulse/goedels -incompleteness -theorem -emergence -ai-eberhard -schoeneburg/
Feature Vector Representation
â—¼Sources of Feature Vector x 
â‘Encoded image
â‘Tabulated data
â‘Embedded words
â‘â€¦
source: Luger (2005)7
Feature Vector Representation
â—¼Sources of Feature Vector x 
â‘Encoded image
â‘Tabulated data
â‘Embedded words
â‘â€¦
source: Luger (2005)
Encoder
 ğ‘µğŸÃ—ğŸ
ğ‘µğ‘µ
89Feature Vector Representation
â—¼Sources of Feature Vector x 
â‘Encoded image
â‘Tabulated data
â‘Embedded words
â‘â€¦
source: Luger (2005)
Encoder
 ğ‘µğŸÃ—ğŸ10Feature Vector Representation
â—¼Sources of Feature Vector x 
â‘Encoded image
â‘Tabulated data
â‘Embedded words
â‘â€¦
source: Luger (2005)
Encoder
 ğ‘µğŸÃ—ğŸ11A Perceptron Network
â—¼Goal: Map Input Feature Vector xinto Output Feature Vector y
12A Perceptron Network
â—¼Goal: Map Input Feature Vector xinto Output Feature Vector y
13A Perceptron Network
â—¼Goal: Map Input Feature Vector xinto Output Feature Vector y
14A Perceptron Network
â—¼Goal: Map Input Feature Vector xinto Output Feature Vector y
15A Perceptron Network
â—¼Goal: Map Input Feature Vector xinto Output Feature Vector y
jâ€™thCell
16A Perceptron Network
â—¼Goal: Map Input Feature Vector xinto Output Feature Vector y
jâ€™thCell
 ?17A Perceptron Network
18A Perceptron Network
19A Perceptron Network
20A Perceptron Network
21A Perceptron Network
22A Perceptron Network
23A Perceptron Network
24A Perceptron Network
25A Perceptron Network
26A Perceptron Network
27Fully Connected (FC) NetworkEncoder
FCğ‘1
ğ‘2
.
.
ğ‘ğ‘€28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47Applications of Neural Networks
â—¼Handwritten digit recognition
â‘Training set = set of handwritten digits (0â€¦9)
â‘Task: given a bitmap, determine what digit it represents
â‘Input: 1 feature for each pixel of the bitmap  
â‘Output: 1 output unit for each possible character (only 1 should 
be activated)
â‘After training, network should work for fonts (handwriting) 
never encountered
â—¼Related pattern recognition
applications : 
â‘recognize postal codes
â‘recognize signatures
â‘â€¦
48Applications of Neural Networks
â—¼Speech synthesis
â‘Learning to pronounce English words
â‘Difficult task for a rule -based system because English 
pronunciation is highly irregular 
â‘Examples:
â—¼letter â€œcâ€ can be pronounced [k] ( cat) or [s] ( cents )
â—¼Woman vs Women
â‘NETtalk:
â—¼uses the context and the letters around a letter to learn how to 
pronounce a letter
â—¼Input: letter and its surrounding letters
â—¼Output: phoneme49
NETtalk Architecture
â—¼Network is made of 3 layers of units
â—¼input unit corresponds to a 7 character window in the text
â—¼each position in the window is represented by 29 input units 
(26 letters + 3 for punctuation and spaces)
â—¼26 output units â€“one for each possible phoneme Ex:  a cat  â†’c is pronounced K
source: Luger (2005)Listen to the output through iterations: https ://www.youtube.com/watch?v=gakJlr3GecE50Neural Networks
â—¼Disadvantage: 
â‘result is not easy to understand by humans (set of 
weights compared to decision tree)â€¦ it is a black box
â—¼Advantage: 
â‘robust to noise in the input (small changes in input do not 
normally cause a change in output) and graceful 
degradation52Today
â—¼Introduction to Neural Networks 
â‘Perceptrons
â‘Backpropagation

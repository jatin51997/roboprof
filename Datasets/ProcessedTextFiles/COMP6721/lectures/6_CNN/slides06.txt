Artificia lIntelligence:
Convolutional Neural Network 
(CNN) Architectures
portion of slides from:  Fei FeiLi
1Today
1.Applications
2.Backbone Models
1.Serial Cascading
2.Serial/Parallel Cascading
3.Residual Connection
4.Depthwise /Separable Convolutions
5.Squeeze -Excitation
2History of AI
3
How Computers Recognize Objects?
4
Question: Object s are anywhere in the scene (in any orientation, 
color hue, perspectives, illumination, etc), so how can we 
recognize them?
Answer: Learn a ton of features (millions) from the bottom up, 
by learning convolutional filters rather than pre -computing themFeature Invariance to Perturbation is Hard
5Viewpoint Variation
(Perspective Geometry)Scale Variation Deformation
Illumination Conditions Background Clutter Intra -Class Variation
ImageNet Large Scale Visual Recognition 
Challenge (ILSVRC)
6ImageNet -1K Class
DogImageNet Large Scale Visual Recognition 
Challenge (ILSVRC)
7ImageNet -1K Class
DogImageNet Large Scale Visual Recognition 
Challenge (ILSVRC)
8ImageNet -1K Class ImageNet -20K Class
Dog
Serial Cascade : AlexNet
9â€¢5 Conv and 3 FC layers
â€¢ReLU Activation
â€¢Training on Multiple GPUs ( GTX 580 with 3GB memory)
â€¢Response Normalization
â€¢Overlapping Pooling
â€¢Heavy data augmentation
â€¢Image translation/horizontal reflection
â€¢Altering intensities of RGBchannels using PCA
10Serial Cascade : AlexNet
ConvolutionConvolutionâ€¦Width 
(ğ‘¾)
Height 
(ğ‘¯)
#Input Channel 
(ğ‘µğ‘°)â€¦Width 
(ğ‘¾â€²)
Height 
(ğ‘¯â€²)
#Output Channel 
(ğ‘µğ‘¶)
ğ‘­ğ‘¶:,:,ğ’‹=à·
ğ’Š=ğŸğ‘µğ‘°
ğ‘­ğ‘°:,:,ğ’Šâˆ—ğ‘²(:,:,ğ’Š,ğ’‹)+ğ’ƒğ’‹ğ‘­ğ‘°âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘° ğ‘­ğ‘¶âˆˆâ„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘¶
ğ‘²âˆˆâ„ğ’‰Ã—ğ’˜Ã—ğ‘µğ‘°Ã—ğ‘µğ‘¶Reminder from Convolution Feature mappingSerial Cascade : AlexNet
11â€¢Feature mapping via cascaded convolutional layers
Serial Cascade : AlexNet
12â€¢AlexNet was the coming out party for CNNs in the computer 
vision community. This was the first time a model performed 
so well on a historically difficult ImageNet dataset.
â€¢How did the learned kernels responses look like?
Serial Cascade : AlexNet
13â€¢Further analysis on AlexNet pretrained kernels (e.g. in 1st
layer) reveals that convolution kernels encode features in 
different orientations, frequencies, and colors
Serial Cascade : AlexNet
14â€¢Further analysis on AlexNet pretrained kernels (e.g. in 1st
layer) reveals that convolution kernels encode features in 
different orientations, frequencies, and colors
Serial Cascade : AlexNet
15
Serial Cascade : Going Deeper with VGG
16â€¢Investigate the effect of the 
convolutional network depth
â€¢Great boost is achieved by increasing 
#layers to 16 -19
â€¢Won ILSVRC 2014 challenge
â€¢Keydesign choice
â€¢3x3kernel size
â€¢Stack of conv layers w/o pooling
â€¢Conv stride =1 (no skipping)
â€¢ReLU activation
â€¢5Max-pooling (x2 downsampling )
â€¢3FClayers
â€¢Later designs added Batch Normalization
Serial Cascade : Going Deeper with VGG
17
Serial Cascade : Going Deeper with VGG
18
Serial Cascade : Going Deeper with VGG
19
Serial Cascade : Going Deeper with VGG
20
Serial Cascade : Going Deeper with VGG
21
Serial Cascade : Going Deeper with VGG
22
Serial Cascade : Going Deeper with VGG
23
Serial Cascade : Going Deeper with VGG â€”
Training Phase
24â€¢Input training image: fixed size of 224x224 crop
â€¢Images have varying size, so upscale to e.g. 384x(N>384)
â€¢Random crop 224x224
â€¢Standard augmentation: random flip and RGB shift
â€¢SGD-Momentum (next lecture)
â€¢Regularization : dropout and weight decay
â€¢Fast convergence (74 training epochs)
â€¢Initialization (some sort of transfer -learning)
â€¢Deeper networks areprone tovanishing -gradients
â€¢11-layer net:random initialization from N(0;0.01)
â€¢Deep er nets: Top & bottom layers initialized with 11 -layer. Other 
layers : random initializationSerial Cascade : Going Deeper with VGG â€”
Testing Phase (ImageNet -1k)
25â€¢Evaluation onvariable sizeimages
â€¢Testing on multiple 224x224 crops [AlexNet ]
â€¢Multiple scales aretested (256xN, 384xN, 512xN) and class score averaged
â€¢Error decreases with depth
â€¢Using multiple scales is important
â€¢Multi-scale training outperforms single scale training =
â€¢Mutli -scale testing further improves the results
Serial/Parallel Cascade : Inception Net
26â€¢Multiple scales are encoded in parallel and cascaded to next 
layers
Residual Connection : ResNet Architecture
27â€¢Is learning better networks as easy as stacking more layers?
â€¢Obstacle: d eeper networks are difficult to train because of the 
notorious problem of vanishing/exploding gradients
â€¢Early solutions were proposed by introducing
â€¢normalized initialization
â€¢intermediate normalization layer
â€¢Still accuracy gets saturated with increasing depth and then degrades 
rapidly (this is not caused by overfitting!)
â€¢Adding more layers leads tohigher training error
Residual Connection : ResNet Architecture
28â€¢Residual learning: instead of hoping each few stacked layers 
directly fit a desired underling mapping, w e explicitly let these 
layers fit a residual mapping
â€¢Define underlying forward mapping by H(x) := F(x) + x
â€¢Corresponding gradient back -propagation:
ğœ•ğœ–
ğœ•ğ‘¥=ğœ•ğœ–
ğœ•ğ»(ğ‘¥)âˆ™ğœ•ğ»ğ‘¥
ğœ•ğ‘¥=ğœ•ğœ–
ğœ•ğ»ğ‘¥âˆ™ğœ•ğ¹
ğœ•ğ‘¥+1=ğœ•ğœ–
ğœ•ğ»ğ‘¥âˆ™ğœ•ğ¹
ğœ•ğ‘¥+ğœ•ğœ–
ğœ•ğ»ğ‘¥
â€¢Gradient from output layer transfers directly to the input layer and avoids 
vanishing
Residual Connection : ResNet Architecture
29â€¢Plain baseline is inspired by VGG nets
â€¢Shortcuts introduced onplain baseline
â€¢Same number of filters are used for the 
same output feature map size
â€¢Ifthefeature map size is halved, the 
number of filters is doubled
â€¢Down -sampling by stride=2
â€¢Network ends with global -average -pooling
â€¢1000 -wayFClayer with softmax
â€¢ResNet34 has3.6BFLOPS (18% of
VGG19 i.e.19.6BFLOPS )
Residual Connection : ResNet Architecture
30â€¢Different ResNet architectures of ResNet18, ResNet34, 
ResNet50, ResNet101, ResNet152
Residual Connection : ResNet Architecture
31â€¢ResNet performance on ImageNet
Depthwise /Separable Convolutions
32â€¢Keep input channel convolution separate from mapping 
to output feature map
Sep-Conv
â€¦
âˆ—Depthwise /Separable Convolutions
33â€¢Keep input channel convolution separate from mapping 
to output feature map
Sep-Conv
â€¦
âˆ—âˆ—Depthwise /Separable Convolutions
34â€¢Keep input channel convolution separate from mapping 
to output feature map
Sep-Conv
â€¦
âˆ—âˆ—âˆ—Depthwise /Separable Convolutions
35â€¢Keep input channel convolution separate from mapping 
to output feature map
Sep-Conv
â€¦
âˆ—âˆ—âˆ—âˆ—â€¦â€¦ğ‘¤3,1Depthwise /Separable Convolutions
36â€¢Keep input channel convolution separate from mapping 
to output feature map
Sep-Conv
â€¦
âˆ—âˆ—âˆ—âˆ—â€¦â€¦Æ©ğ‘¤2,1ğ‘¤1,1
ğ‘¤ğ‘ğ¼,1ğ‘¤3,2Depthwise /Separable Convolutions
37 Sep-Conv
â€¦
âˆ—âˆ—âˆ—âˆ—â€¦â€¦Æ©ğ‘¤2,2ğ‘¤1,2
ğ‘¤ğ‘ğ¼,2â€¢Keep input channel convolution separate from mapping 
to output feature mapğ‘¤3,3Depthwise /Separable Convolutions
38 Sep-Conv
â€¦
âˆ—âˆ—âˆ—âˆ—â€¦â€¦Æ©ğ‘¤2,3ğ‘¤1,3
ğ‘¤ğ‘ğ¼,3â€¢Keep input channel convolution separate from mapping 
to output feature mapğ‘¤3,ğ‘ğ‘‚Depthwise /Separable Convolutions
39 Sep-Conv
â€¦
âˆ—âˆ—âˆ—âˆ—â€¦â€¦ â€¦Æ©ğ‘¤2,ğ‘ğ‘‚ğ‘¤1,ğ‘ğ‘‚
ğ‘¤ğ‘ğ¼,ğ‘ğ‘‚â€¢Keep input channel convolution separate from mapping 
to output feature mapDepthwise /Separable Convolutions
40 Sep-Conv
â€¦ğ‘­ğ‘°âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘°â€¢Keep input channel convolution separate from mapping 
to output feature mapDepthwise /Separable Convolutions
41 Sep-Conv
â€¦ â€¦ğ‘­ğ‘°âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘° à´¥ğ‘­ğ‘°âˆˆâ„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘°à´¥ğ‘²âˆˆâ„ğ’‰Ã—ğ’˜Ã—ğŸÃ—ğ‘µğ‘°â€¢Keep input channel convolution separate from mapping 
to output feature mapDepthwise /Separable Convolutions
42 Sep-Conv
â€¦ â€¦ â€¦ğ‘­ğ‘°âˆˆâ„ğ‘¯Ã—ğ‘¾Ã—ğ‘µğ‘° à´¥ğ‘­ğ‘°âˆˆâ„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘° ğ‘­ğ‘¶âˆˆâ„ğ‘¯â€²Ã—ğ‘¾â€²Ã—ğ‘µğ‘¶à´¥ğ‘²âˆˆâ„ğ’‰Ã—ğ’˜Ã—ğŸÃ—ğ‘µğ‘°
à´¥ğ‘²âˆˆâ„ğŸÃ—ğŸÃ—ğ‘µğ‘°Ã—ğ‘µğ‘¶â€¢Keep input channel convolution separate from mapping 
to output feature mapDepthwise /Separable Convolutions
43 Sep-Convâ€¢What arethebenefits?
â€¢Standard convolution hasthecomputation cost of
â€¢Depthwise -separable convolution has the computation cost of
â€¢Reduction in computation ratioğ‘¶ğ’‰âˆ™ğ’˜âˆ™ğ‘µğ‘°âˆ™ğ‘µğ‘¶âˆ™ğ‘¯âˆ™ğ‘¾
Convolution 
KernelInput 
Feature Map
ğ‘¶ğ’‰âˆ™ğ’˜âˆ™ğ‘µğ‘°âˆ™ğ‘¯âˆ™ğ‘¾+ğ‘µğ‘°âˆ™ğ‘µğ‘¶âˆ™ğ‘¯âˆ™ğ‘¾
Conv 
Kernel -1Input 
FeatureConv 
Kernel -2Input 
Feature
ğ’“=ğ’‰ğ’˜ğ‘µğ‘°ğ‘¯ğ‘¾+ğ‘µğ‘°ğ‘µğ‘¶ğ‘¯ğ‘¾
ğ’‰ğ’˜ğ‘µğ‘°ğ‘µğ‘¶ğ‘¯ğ‘¾=ğŸ
ğ‘µğ‘¶+ğŸ
ğ’‰ğ’˜
ğ‘µğ‘¶
ğ’‰ğŸ
ğ’“Depthwise /Separable Convolutions
44 Sep-Convâ€¢Activation and Batch -Normalization are used inbetween
â€¢1x1convolution also referred by Pointwise Convolution
MobileNet by Depthwise Separable
Convolutions
45â€¢All layers are followed 
by BN and ReLU
â€¢MobileNet has 28 
separate layers
MobileNet by Depthwise Separable
Convolutions
46â€¢Significant parameter reduction while maintaining performance 
accuracy
Squeeze -Excitation Block
47â€¢The idea is to boost the representation power of the network
â€¢SE:channel relationships are adaptively recalibrated in 
channel -wise feature response by explicit modelling 
interdependencies between channels
Squeeze -Excitation Block
48
Today
1.Applications
2.Backbone Models
1.Serial Cascading
2.Serial/Parallel Cascading
3.Residual Connection
4.Depthwise /Separable Convolutions
5.Squeeze -Excitation
49

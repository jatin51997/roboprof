1Artificial Intelligence:
Introduction to  
Natural Language ProcessingMenu
1.Introduction
2.Bag of word model
3.n-gram models
4.Linguistic features for NLP
â—¼2
Languages
â—¼Artificial
â‘Smaller vocabulary
â‘Simple syntactic structures
â‘Non-ambiguous
â‘Not tolerant to errors (ex. Syntax error)
â—¼Natural
â‘Large and open vocabulary (new words everyday)
â‘Complex syntactic structures
â‘Very ambiguous
â‘Robust (ex. forgot a comma, a wordâ€¦ still OK)
â—¼3Question Answering: IBMâ€™s Watson
â—¼Won Jeopardy on February 16, 2011!
4WILLIAM WILKINSONâ€™S 
â€œAN ACCOUNT OF THE PRINCIPALITIES OF
WALLACHIA AND MOLDOVIAâ€
INSPIRED THIS AUTHORâ€™S
MOST FAMOUS NOVELWho is Bram 
Stoker?
(Dracula)
Information Extraction
Subject: curriculum meeting
Date: January 15, 2012
To: Dan Jurafsky
Hi Dan, weâ€™ve now scheduled the curriculum meeting.
It will be in Gates 159 tomorrow from 10:00 -11:30.
-Chris
Create new Calendar entry
Event: Curriculum mtg
Date: Jan-16-2012
Start:   10:00am
End:    11:30am
Where: Gates 159Information Extraction & Sentiment Analysis
nice and compact to carry! 
since the camera is small and light, I won't need to carry around 
those heavy, bulky professional cameras either! 
the camera feels flimsy, is plastic and very light in weight you have 
to be very delicate in the handling of this camera
6
Size and weightAttributes :
zoom
affordability
size and weight
flash 
ease of use
âœ“
âœ—âœ“
slide from Olga Veksler (U. Western Ontario)Machine Translation
Fully automatic
7
Helping human translators
Enter Source Text:
Translation from Stanfordâ€™s Phrasal :è¿™ä¸è¿‡æ˜¯ä¸€ä¸ªæ—¶é—´çš„é—®é¢˜ .
This is only a matter of time.
slide from Olga Veksler (U. Western Ontario)Where we are today
Part-of-speech (POS) tagging
Named entity recognition (NER)Sentiment analysismostly solved making good progressGood progress by 
Deep Learning
Spam detection
Letâ€™s go to Agra!
Buy V1AGRA â€¦âœ“
âœ—
Colorless   green   ideas   sleep   furiously.
ADJ         ADJ    NOUN  VERB      ADV
Einstein met with UN officials in Princeton
PERSON              ORG                      LOC
Information extraction (IE)
Youâ€™re invited to our 
dinner party, Friday May 
27 at 8:30
Party
May 
27
add
Best roast chicken in San Francisco!
The waiter ignored us for 20 minutes.
Machine translation (MT)
The 13thShanghai International Film Festivalâ€¦
ç¬¬13å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹• â€¦
Question answering (QA)
Q. How effective is ibuprofen in 
reducing fever in patients with acute 
febrile illness?
Parsing
I can see Alcatraz from the window!
Paraphrase
XYZ acquired ABC yesterday
ABC has been taken over by XYZ
Summarization
The Dow Jones is up
Housing prices rose
Economy 
is good
The S&P500 jumped
Coreference resolution
Carter told Mubarak he shouldnâ€™t run again.
Word sense disambiguation (WSD)
I need new batteries for my mouse .
Dialog
 Where is Citizen Kane playing 
in SF? 
Castro Theatre at 7:30. 
Do you want a ticket?
slide from Olga Veksler (U. Western Ontario)â—¼Because it is ambiguous:
1.The computer understands you 
as well as your mother 
understands you.
1.The computer understands that 
you like (love) your mother.
1.The computer understands you 
as well as it understands your 
mother.Why is NLP hard?
â€œAt last, a computer that understands you like your motherâ€        
slide from Olga Veksler (U. Western Ontario)10Another Example of Ambiguity
â‘Even simple sentences are highly ambiguous
â‘â€œGet the cat with the gloves â€
slide from Olga Veksler (U. Western Ontario)11And Even More Examples of Ambiguity
â—¼Iraqi Head Seeks Arms
â—¼Ban on Nude Dancing on Governorâ€™s Desk
â—¼Juvenile Court to Try Shooting Defendant
â—¼Teacher Strikes Idle Kids
â—¼Kids Make Nutritious Snacks
â—¼British Left Waffles on Falkland Islands
â—¼Red Tape Holds Up New Bridges
â—¼Bush Wins on Budget, but More Lies Ahead
â—¼Hospitals are Sued by 7 Foot Doctors
â—¼Stolen Painting Found by Tree
â—¼Local HS Dropouts Cut in Half
slide from Olga Veksler (U. Western Ontario)â—¼Natural Language Processing
= automatic processing of written texts
1.Natural Language Understanding
â‘Input = text
2.Natural Language Generation
â‘Output = text
â—¼Speech Processing
= automatic processing of speech
1.Speech Recognition 
â‘Input = acoustic signal
2.Speech Synthesis
â‘Output = acoustic signalNLP vs Speech Processing
â—¼12
Remember these slides?
13
The Ancient Land of NLP (aka GOFAI)
(circa A.D. 1950...mid 1980)
14
The Ancient Land of NLPSpeech 
Kingdom Village of 
CS & 
LinguistsInformation 
Retrieval ForestMachine 
Learning 
Island
Rule-based NLP
(circa A.D. 1950...mid 1980)
15
https://image.slidesharecdn.com/maryamsaihbani -161004094341/95/lefttoright -hierarchical -phrasebased -
translation -and-its-application -in-simultaneous -speech -translation -maryam -siahbani -4-638.jpg?cb=1477004310Symbolic methods / Linguistic approach / Knowledge -rich approach
â€¢ Cognitive approach
â€¢ Rules are developed by hand in collaboration with linguists1stInvasion of NLP, from ML
(mid 1980 â€“circa 2010)
16
The Land of Statistical NLPSpeech 
KingdomCity of 
CS& 
LinguistsInformation 
Retrieval ForestMachine 
Learning 
Island
Statistical NLP
(mid 1980 â€“circa 2010)
17
Syntactic 
parsingPart-of-
speech 
taggingstemmingtokenisation Decision trees
Statistical methods / Machine Learning / Knowledge -poor method
â€¢Engineering Approach
â€¢Rules are developed automatically (using machine learning)
â€¢But the linguistic features are hand -engineered and fed to the ML model  
â€¢Applications: Information Retrieval, Predictive Text / Word Completion, 
Language Identification, Text Classification, Authorship Attribution...Neural networks
NaÃ¯ve Bayes classifier
K-means clusteringFeature Extraction 
(designed by hand)Machine Learning 
Model ApplicationsStatistical NLP
(mid 1980 â€“circa 2010)
18Applications
linguistic features are hand -engineered and fed to the ML model  2ndInvasion of NLP, by Deep Learning
(circa 2010 -today)
19
The Modern Land of 
Deep Language ProcessingSpeech 
KingdomMetropolis 
of  Deep 
Language 
ProcessingInformation 
Retrieval ForestDeep
Learning 
Island
Deep Language Processing
(circa 2010 -today)
20Deep Neural Networks applied to NLP problems
â€¢Rules are developed automatically (using machine learning)
â€¢And the linguistic features are found automatically!Applications
Menu
â—¼21
1.Introduction
2.Bag of word model
3.n-gram models
4.Linguistic features for NLP22Bag-of-word Model (BOW)
â—¼A simple model where word order is ignored
â—¼used in many applications:
â‘NB spam filter seen in class a few weeks ago
â‘Information Retrieval (eg. google search)
â‘...
â—¼But has severe limits to understand meaning of text...
â—¼Maybe we should take word order into account...
Word Freq.
Mary 2
apples 1
did 2
eat 1
John 1
kill 1
like 1
not 1
to 123Limits of BOW Model
â—¼word order is ignored ==> meaning of text is lost.
â—¼n-grams take [a bit of] word order into account
Word Freq.
Mary 2
apples 1
did 2
eat 1
John 1
kill 1
like 1
not 1
to 1Mary did kill John.
Mary did not like to eat 
apples.
John did not kill Mary.
Mary did like to eat apples.
Mary did not like to kill 
John.
Mary did eat apples.
...Menu
â—¼24
1.Introduction
2.Bag of word model
3.n-gram models
4.Linguistic features for NLP25n-gram Model
â—¼An n-gram model is a probability distribution over sequences of 
events (grams/units/items)
â—¼models the order of the events
â—¼Used when the past sequence of events is a good indicator of 
the next event to occur in the sequence
â—¼i.e. To predict the next event in a sequence of event 
â—¼E.g.: 
â‘next move of player based on his/her past moves
â—¼ left right right up ... up? down? left? right?
â‘next base pair based on past DNA sequence 
â—¼ AGCTTCG ... A? G? C? T?
â‘next word based on past words 
â—¼ Hi dear, how are ... helicopter? laptop? you? magic?26Whatâ€™s a Language Model?
â—¼A Language model is a n -gram model over word/character 
sequences
â—¼ie: events = words  or  events = character
â—¼P(â€œIâ€™d like a coffee with 2 sugars and milkâ€) â‰ˆ 0.001
â—¼P(â€œIâ€™d hike a toffee with 2 sugars and silkâ€) â‰ˆ 0.00000000127Applications of Language Models
â—¼Speech Recognition
â—¼Statistical Machine Translation
â—¼Language Identification
â—¼Spelling correction 
â‘He is trying to fineout. 
â‘He is trying to findout. 
â—¼Optical character recognition / Handwriting 
recognition
â—¼â€¦In Speech Recognition
â–ªGoal: find most likely sentence (S*) given the observed sound (O) â€¦ 
â–ªie. pick the sentence with the highest probability:
â—¼We can use Bayes rule to rewrite this as:
â—¼Since denominator is the same for each candidate S, we can ignore it for the 
argmax:
Acoustic model --
Probability of the possible 
phonemes in the language + 
Probability of â‰  pronunciations Language model --P(a sentence) 
Probability of the candidate 
sentence in the language
Given: Observed sound -O
Find: The most likely word/sentence â€“S*
S1: How to recognize speech .  ?
S2: How to wreck a nice beach . ?
S3: â€¦
O)|P(S argmax S*
LSïƒ=
P(S)S)|P(O argmaxS*
LSï‚´ =
ïƒ
P(O)S)P(S)|P(Oargmax S*
LSïƒ=29In Speech Recognition
argmaxP(acoustic signal)P(acoustic signal | word sequence) x P(word sequence)argmaxP(word sequence | acoustic signal) argmax
ce wordsequence wordsequence word sequencAcoustic modelLanguage model
P(acoustic signal | word sequence) x P(word sequence)
P(S)S)|P(O argmax S*
LSï‚´ =
ïƒIn Statistical Machine Translation
â—¼Assume we translate from fr [foreign ] to English  i.e.: (en|fr)
Given: Foreign sentence -fr
Find: The most likely English 
sentence â€“en*
S1: Translate that!
S2: Translated this! 
S3: Eat your soup!
S4â€¦
Translation modelLanguage modelAutomatic Language Identificationâ€¦ 
guess how thatâ€™s done?
P(en) x en)|P(fr argmax en*
en=31â€œShannon Gameâ€ (Shannon, 1951)
â€œI am going to make a collect â€¦â€
â—¼Predict the next word/character given the n-1previous 
words/characters.
https://en.wikipedia.org/wiki/Claude_Shannon321stapproximation
â—¼each word has an equal probability to follow any 
other
â‘with 100,000 words, the probability of each word at any 
given point is .00001 
â—¼but some words are more frequent then othersâ€¦
â—¼â€œtheâ€ appears many more times, than â€œ rabbitâ€332ndapproximation: unigrams
â—¼take into account the frequency of the word in 
some training corpus
â‘at any given point, â€œ theâ€ is more probable than â€œ rabbitâ€
â—¼but does not take word order into account.  This 
is the bag of word approach. 
â‘â€œJust then, the white â€¦â€
â—¼so the probability of a word also depends on the 
previous words (the history)
P(wn |w1w2â€¦wn-1)34n-grams
â—¼â€œthe large green ______ .â€
â‘â€œmountain â€? â€œtreeâ€? 
â—¼â€œSue swallowed the large green ______ .â€
â‘â€œpillâ€?  â€œbroccoli â€?
â—¼Knowing that Sue â€œ swallowed â€ helps narrow down 
possibilities 
â—¼i.e., going back 3 words before helps
â—¼But, how far back do we look?35Bigrams
â—¼first-order Markov models
â—¼N-by-N matrix of probabilities/frequencies 
â—¼N = size of the vocabulary we are usingP(wn|wn-1)1stword2ndword
 a aardvark  aardwolf  aback â€¦ zoophyte  zucchini  
a 0 0 0 0 â€¦ 8 5 
aardvark  0 0 0 0 â€¦ 0 0 
aardwolf  0 0 0 0 â€¦ 0 0 
aback 26 1 6 0 â€¦ 12 2 
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 
zoophyte  0 0 0 1 â€¦ 0 0 
zucchini  0 0 0 3 â€¦ 0 0 
 
 36Trigrams
â—¼second -order Markov models
â—¼N-by-N-by-N matrix of probabilities/frequencies 
â—¼N = size of the vocabulary we are usingP(wn|wn-1wn-2)1stword
2ndword3rdword37Why use only bi -or tri -grams? 
â—¼Markov approximation is still costly
with a 20 000 word vocabulary:
â‘bigram needs to store 400 million parameters
â‘trigram needs to store 8 trillion parameters
â‘using a language model > trigram is impractical38Building n -gram Models
1.Data preparation: 
â‘Decide on training corpus
â‘Clean and tokenize
â‘How do we deal with sentence boundaries? 
â—¼I eat.  I sleep.    
â‘(I eat) (eat I) (I sleep) 
â—¼<s>I eat </s> <s> I sleep </s> 
â‘(<s> I) (I eat) (eat </s>) (<s> I) (I sleep) (sleep </s>)39Example 1: 
â—¼in a training corpus, we have 10 instances of 
â€œcome acrossâ€
â‘8 times, followed by â€œ asâ€
â‘1 time, followed by â€œ moreâ€
â‘1 time, followed by â€œ aâ€
â—¼so we have: 
â‘
â‘P(more | come across) = 0.1 
â‘P(a | come across) = 0.1 
â‘P(X | come across) = 0  where X â‰ â€œasâ€, â€œmoreâ€, â€œaâ€
108
across) C(comeas) across C(come  across) come |P(as = =40Building n -gram Models
2.Count words and build model
â‘Let C(w1...wn) be the frequency of n -gram w1...wn
3.Smooth your model (see later)
) ...wC(w)...wC(w  ) ...ww|(wP
1-n 1n 1
1-n 1 n =41Example 2:
P(I want to eat British food) 
= P(I|<s>) x P(want|I) x P(to|want) x P(eat|to) x P(British|eat) x P(food|British)
= .25       x .32           x .65             x .26          x .001                x .6
= .000008
P(on|eat) =   .16 
P(some|eat) =  .06 
P(British|eat) =  .001 
â€¦ 
P(I|<s>) =  .25 
P(Iâ€™d| <s>) =   .06 
â€¦ P(want| I) =   .32 
P(would |I) =  .29 
P(donâ€™t |I) =   .08 
â€¦ 
P(to| want) =  .65 
P(a|want) =   .5 
â€¦ P(eat|to) =    .26 
P(have|to) =    .14 
P(spend |to)=    .09 
â€¦ 
P(food|British) =   .6 
P(restaurant| British ) = .15 
â€¦ 
 
 Remember this slide...
42
43Some Adjustments
â—¼product of probabilitiesâ€¦ numerical underflow 
for long sentences
â—¼so instead of multiplying the probs, we add the 
log of the probs
P(I want to eat British food) 
= log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) + 
log(P(British|eat)) + log(P(food|British))
= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)44Problem: Data Sparseness  
â—¼What if a sequence never appears in training corpus? P(X)=0
â‘â€œcome across the menâ€ -->prob = 0  
â‘â€œcome across some menâ€ --> prob = 0
â‘â€œcome across 3 menâ€ -->prob = 0
â—¼The model assigns a probability of zero to unseen events â€¦ 
â—¼probability of an n -gram involving unseen words will be zero!
â—¼Solution: smoothing
â‘decrease the probability of previously seen events 
â‘so that there is a little bit of probability mass left over for 
previously unseen events Remember this other slide...
45
46Add-one Smoothing
â—¼Pretend we have seen every n -gram at least once 
â—¼Intuitively:
â‘new_count(n -gram) = old_count(n -gram) + 1
â—¼The idea is to give a little bit of the probability 
space to unseen events47Add-one: Example
unsmoothed bigram counts (frequencies):1stword2ndword
â—¼Assume a vocabulary of 1616 (different) words
â‘V = {a, aardvark, aardwolf, aback, â€¦ , I, â€¦, want,â€¦ to, â€¦, eat, Chinese, â€¦, food, â€¦, lunch, â€¦,    
zoophyte, zucchini}
â‘|V| = 1616 words
â—¼And a total of N = 10,000 bigrams (~word instances) in the training corpus
 I want to eat Chinese  food lunch  â€¦ Total 
I 8 1087  0 13 0 0 0  C(I)=3437  
want 3 0 786 0 6 8 6  C(want)= 1215 
to 3 0 10 860 3 0 12  C(to)= 3256  
eat 0 0 2 0 19 2 52  C(eat)= 938 
Chinese  2 0 0 0 0 120 1  C(Chinese)= 213 
food 19 0 17 0 0 0 0  C(food)= 1506  
lunch 4 0 0 0 0 1 0  C(lunch)= 459 
â€¦         ... 
         ... 
         N=10,000  
 
 48Add-one: Example
unsmoothed bigram counts :
unsmoothed bigram conditional probabilities :1stword2ndword
 I want to eat Chinese  food lunch  â€¦ Total 
I 8 1087  0 13 0 0 0  C(I)=3437  
want 3 0 786 0 6 8 6  C(want)= 1215 
to 3 0 10 860 3 0 12  C(to)= 3256  
eat 0 0 2 0 19 2 52  C(eat)= 938 
Chinese  2 0 0 0 0 120 1  C(Chinese)= 213 
food 19 0 17 0 0 0 0  C(food)= 1506  
lunch 4 0 0 0 0 1 0  C(lunch)= 459 
â€¦          
         N=10,000  
 
 
437 38I)|P(I000 108P(II): note
==
49Add-one, more formally
N: size of the corpus
i.e. nb of n -gram tokens in training corpus 
B: number of "bins"
i.e. nb of different n-gram types
i.e. nb of cells in the matrix
e.g. for bigrams, it's (size of the vocabulary)2
B  N1  )w w (w C  )w w (wPn 12
n 21 Add1++ ï‚¼= ï‚¼50Add-one: Example (conâ€™t)
add-one smoothed bigram counts :
add-one bigram conditional probabilities :
 I want to eat Chinese  food lunch  â€¦ Total 
I 8   9 1087  
1088  1 14 1 1 1  3437   
C(I) + |V| = 5053  
want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831  
to 4 1 11 861 4 1 13  C(to) + |V| = 4872  
eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554  
Chinese  3 1 1 1 1 121 2  C(Chinese) + |V| = 1829  
food 20 1 18 1 1 1 1  C(food) + |V| = 3122  
lunch  5 1 1 1 1 2 1  C(lunch) + |V| = 2075  
â€¦         total = 10,000  
N+|V|2 = 10,000 + ( 1616)2 
= 2,621,456   
 
 I want to eat Chinese  food lunch  â€¦ 
I .0018  
(9/5053 ) .215 .00019  .0028  
 .00019  .00019  .00019   
want .0014  .00035  .278 .00035  .0025  .0031  .00247   
to .00082  .0002  .00226  .1767  .00082  .0002  .00267  
eat .00039  .00039  .0009  .00039  .0078  .0012  .0208  
â€¦         
 
 51Add-delta Smoothing
â—¼every previously unseen n -gram is given a low probability
â—¼but there are so many of them that too much probability mass is 
given to unseen events
â—¼instead of adding 1, add some other (smaller) positive value ğ›¿
â—¼most widely used value for ğ›¿= 0.5
â—¼better than add -one, but stillâ€¦
B   N  )w w (w C  )w w (wPn 12
n 21 AddDï¤ï¤
++ ï‚¼= ï‚¼52Factors of Training Corpus
â—¼Size: 
â‘the more, the better
â‘but after a while, not much improvementâ€¦
â—¼bigrams (characters) after 100â€™s million words
â—¼trigrams (characters) after some billions of words
â—¼Genre (adaptation):
â‘training on cooking recipes and testing on aircraft 
maintenance manuals53Example: Language Identification
â—¼hypothesis: texts that resemble 
each other (same author, same 
language) share similar 
character/word sequences  
â‘In English character sequence 
â€œingâ€ is more probable than in 
French  
â—¼Training phase: 
â‘construction of the language 
model 
â‘with pre -classified documents 
(known language/author)
â—¼Testing phase: 
â‘apply language model to unknown 
textAutomatic Language Identificationâ€¦ 54Example: Language Identification
â—¼bigram of characters 
â‘characters = 26 letters (case insensitive)
â‘possible variations: case sensitivity, 
punctuation, beginning/end of sentence 
marker, â€¦551. Train a character -based language model for Italian:
2. Train a character -based language model for Spanish:
3. Given a unknown sentence â€œche bella cosaâ€  is it in Italian or in Spanish?
P(â€œche bella cosaâ€) with the Italian LM
P(â€œche bella cosaâ€) with the Spanish LM
4. Highest probability â†’language of sentenceExample: Language Identification
 A B C D â€¦ Y Z 
A 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
B 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
C 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
D 0.0042  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
E 0.0097  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 0.0014  
Y 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
Z 0.0014  0.0014  0.0014  0.0014  0.0014  0.0014  0.0014  
 
 
 A B C D â€¦ Y Z 
A 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
B 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
C 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
D 0.0042  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
E 0.0097  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 0.0014  
Y 0.0014  0.0014  0.0014  0.0014  â€¦ 0.0014  0.0014  
Z 0.0014  0.0014  0.0014  0.0014  0.0014  0.0014  0.0014  
 
 Googleâ€™s Web 1T 5 -gram model
â—¼5-grams
â—¼generated from 1 trillion words
â—¼24 GB compressed 
â‘Number of tokens: 1,024,908,267,229 
â‘Number of sentences: 95,119,665,584 
â‘Number of unigrams: 13,588,391 
â‘Number of bigrams: 314,843,401 
â‘Number of trigrams: 977,069,902 
â‘Number of fourgrams: 1,313,818,354 
â‘Number of fivegrams: 1,176,470,663
â—¼See discussion: http://googleresearch.blogspot.com/2006/08/all -our-n-gram -are-belong -
to-you.html
â—¼See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer
56Problem with n -grams
â—¼Natural language is not linear ....
â—¼there may be long-distance dependencies .
â‘Syntactic dependencies
â—¼The mannext to the large oak tree near â€¦ istall.
â—¼The mennext to the large oak tree near â€¦ aretall.
â‘Semantic dependencies
â—¼The birdnext to the large oak tree near â€¦ flies rapidly.
â—¼The mannext to the large oak tree near â€¦ talks rapidly.
â‘World knowledge
â—¼Michael Jackson , who was featured in ..., is buried in California.
â—¼Michael BublÃ© , who was featured in ..., is living in California.
â‘...
â—¼More complex models of language are needed to handle such 
dependencies.
57Menu
â—¼58
1.Introduction
2.Bag of word model
3.n-gram models
4.Linguistic features for NLPLinguistic features used for what?
59Applications
linguistic features are hand -engineered and fed to the ML model  60Stages of NLU
source: Luger (2005)61Stages of NLU
source: Luger (2005)Parsing (Syntax): 
â—¼What words are available in a 
language?  gfiioudd  / table
â—¼How to arrange words 
together? 
the rose is red / red the rose is
Semantic interpretation: 
â—¼ Lexical Semantics : 
What is the 
meaning/semantic 
relations between 
individual words? 
Chair:  person?  Furniture?
â—¼ Compositional 
Semantics: What is the 
meaning of phrases and 
sentences?
The chairâ€™s leg is broken
62Stages of NLU
source: Luger (2005)â—¼ Discourse Analysis
How to relate the meaning of sentences to 
surrounding sentences?
I have to go to the store.  I need butter.
I have to go to the university.  I need butter.
â—¼ Pragmatics
How people use language in a social 
environment?
Do you have a child?    
Do you have a quarter?
â—¼ World Knowledge 
How knowledge about the world (history, 
facts, â€¦) modifies our understanding of 
text?
Bill Gates passed away last night.63Stages of NLU
source: Luger (2005)
Parsing (Syntax): 
â—¼What words are available in a 
language?  gfiioudd  / table
â—¼How to arrange words 
together? 
the rose is red / red the rose is64Syntactic Parsing
1.Assign the right part of speech (NOUN, VERB, â€¦) to 
individual words in a text
1.Determine how words are put together to form 
correct sentences
â—¼The/DET rose/NOUN is/VERB red/ADJ.  
â—¼Is/VERB red/ADJ the/DET rose/NOUN.65English Parts -of-Speech
â—¼Open (lexical) class words 
â‘new words can be added easily
â‘nouns, main verbs, adjectives, adverbs
â‘some languages do not have all these categories
â—¼Closed (functional) class words
â‘generally function/grammatical words 
â‘aka stop words
â‘ex. the, in, and, over, beyondâ€¦
â‘relatively fixed membership
â‘prepositions, determiners, pronouns, conjunctions, â€¦
Smurf talk on youtube:
https ://www.youtube.com/watch?v=7BPx -vl8G0066Syntax
â—¼How parts -of-speech are organised into larger 
syntactic constituents
â—¼Main Constituents:
â‘S: sentence The boy is happy.
â‘NP: noun phrase      the little boy from Paris, Sam Smith, I, 
â‘VP: verb phrase eat an apple, sing, leave Paris in the night 
â‘PP: prepositional phrase  in the morning, about my ticket
â‘AdjP: adjective phrase really funny, rather clear
â‘AdvP: adverb phrase slowly, really slowly67A Parse Tree
â—¼a tree representation of the application 
of the grammar to a specific sentence.
68a CFG consists of
â—¼set of non -terminal symbols 
â‘constituents & parts -of-speech
â‘S, NP, VP, PP, D, N, V, ...
â—¼set of terminal symbols 
â‘words & punctuation
â‘cat, mouse, nurses, eat, ...
â—¼a non -terminal designated as the starting symbol 
â‘sentence S
â—¼a set of re -write rules 
â‘having a single non -terminal on the LHS and one or more 
terminal or non -terminal in the RHS
â‘S --> NP VP 
â‘NP --> Pro 
â‘NP --> PN 
â‘NP --> D N69An Example
â—¼Lexicon:
N --> flight | trip | breeze | morning // noun
V --> is | prefer | like // 
verb
Adj --> direct | cheapest | first // 
adjective
Pro --> me | I | you | it // 
pronoun
PN --> Chicago | United | Los Angeles // proper noun
D --> the | a | this // 
determiner
Prep --> from | to | in // 
preposition
Conj --> and | or | but // 
conjunction
â—¼Grammar:
S --> NP VP // I + 
prefer United
NP --> Pro | PN | D N | D Adj N   // I, Chicago, the morning
VP --> V | V NP | V NP PP // is, prefer + 
United, 
PP --> Prep NP // 
to Chicago, to I ??70Parsing
â—¼parsing: 
â‘goal: 
â—¼assign syntactic structures to a sentence 
â‘result: 
â—¼(set of) parse trees
â—¼we need:
â‘a grammar: 
â—¼description of the language constructions
â‘a parsing strategy: 
â—¼how the syntactic analysis are to be computed 71Parsing Strategies
â—¼parsing is seen as a search problem 
through the space of all possible parse 
trees
â‘bottom -up (data -directed): words -->grammar
â‘top-down (goal -directed): grammar -->words
â‘breadth -first: compute all paths in parallel
â‘depth -first: exhaust 1 path before considering 
another
â‘Heuristic search72Example: John ate the cat
â—¼Bottom -up parsing / 
breadth first
1.John ate the cat
2.PN ate the cat
3.PN V the cat
4.PN V ART cat
5.PN V ART N
6.NP V ART N
7.NP V NP
8.NP VP
9.Sâ—¼Top-down parsing / 
depth first
1.S
2.NP VP
3.PN VP
4.John VP
5.John V NP
6.John ate NP 
7.John ate ART N
8.John ate the N
9.John ate the cat73Depth -first vs Breadth -first 
the cat eats the mouse.
â—¼depth -first: exhaust 1 path 
before considering another
â—¼breadth -first: 
â‘compute 1 level at a time
â—¼Heuristic search: 
â‘e.g. preference to shorter rulesGrammar:
(1) S --> NP VP
(2) S --> VP
(3) S --> Aux NP VP 
(4) NP --> Det N PP
(5) NP --> Det N
(6) PP --> Prep N
â€¦
Lexicon:
(10) Det --> the
(11) N --> cat
(12) VB --> eats
â€¦
S 
 
 
NP-VP       VP   Aux-NP-VP 
 
Det-N-PP Det-N â€¦ 
 
the  cat  Prep-NP 
 
 74Summary of Parsing Strategies
 Depth 
First Breath  
First Heuristic 
Search  
Top down âœ“ âœ“ âœ“ 
Bottom up âœ“ âœ“ âœ“ 
 
 75Problem: Multiple parses
â—¼Many possible parses for a single sentence happens 
very oftenâ€¦ 
â‘Prepositional phrase attachment (PP -attachment)
â—¼We painted the wall with cracks.
â—¼The man saw the boy with the telescope.
â—¼I shot an elephant in my pyjamas.
â‘Conjunctions and appositives
â—¼Maddy, my dog, and Samy
--> (Maddy, my dog), and (Samy)
--> (Maddy), (my dog), and (Samy)
â—¼These phenomena can quickly increase the number of 
possible parse trees!76PP attachment:
The man saw the boy with the telescope.
Correct parse 1 Correct parse 2
source: Robert Dale.77
Probabilistic Parsing
â€œOne morning I shot an elephant in my pyjamas.  How he got 
into my pyjamas, I donâ€™t know.â€
G. Marx, Animal Crackers , 1930 .
â—¼Sentences can be very ambiguousâ€¦ 
â‘A non -probabilistic parser may find a large set of possible 
parses
â‘--> need to pick the most probable parse one from the set 78Example of a PCFG
â—¼Intuitively, P(VP â†’V NP) is:
â‘the probability of expanding VP by a V NP, as opposed 
to any other rules for VP
â—¼So for:
â‘VP: âˆ€i âˆ‘iP(VP --> B) = .7 + .3 = 1
â‘NP: âˆ€i âˆ‘iP(NP --> B) = .4 + .1 + .18 + .04 + .18 + .1 = 1
source:  Manning, and SchÃ¼tze, Foundations of Statistical Natural Language Processing, MIT Press (1999)79â—¼Product of the probabilities of the rules used in subtrees
â—¼Ex: â€œAstronomers saw stars with ears.â€
. 
P(t1) = 1x.1x.7x1x.4x.18x1x1x.18 P(t2) = 1x.1x.3x.7x1x1x.18x1x.18
= .0009072 = .0006804Probability of a parse tree
source:  Manning, and SchÃ¼tze, Foundations of Statistical Natural Language Processing, MIT Press (1999)80Stages of NLU
source: Luger (2005)
Semantic interpretation: 
â—¼ Lexical Semantics : 
What is the 
meaning/semantic 
relations between 
individual words? 
Chair:  person?  Furniture?
â—¼ Compositional 
Semantics: What is the 
meaning of phrases and 
sentences?
The chairâ€™s leg is broken81Semantic Interpretation
â—¼Map sentences to some representation of its 
meaning
â‘e.g., logics, knowledge graph, embeddingâ€¦
1.Lexical Semantics 
â—¼i.e., Meaning of individual words
1.Compositional Semantics
â—¼i.e., Meaning of combination of words82Lexical Semantics
â—¼ie. The meaning of individual words
â‘A word may denote different things (ex. chair)
â‘The meaning/sense of words is not clear -cut
â‘E.g. Overlapping of word senses across languages
legpatte
Ã©tape
jambe piedanimal
journey
humanchair83Word Sense Disambiguation (WSD)
â—¼Determining which sense of a word is used 
in a specific sentence
â‘I went to the bank of Montreal and deposited 50$.
â‘I went to the bank of the river and dangled my feet .
84â—¼WSD can be viewed as typical classification 
problem
â‘use machine learning techniques (ex. NaÃ¯ve Bayes 
classifier, decision tree) to train a system
â‘that learns a classifier (a function f) to assign to 
unseen examples one of a fixed number of senses 
(categories)
â—¼Input: 
â‘Target word: The word to be disambiguated 
â‘Features? 
â—¼Output: 
â‘Most likely sense of the wordWSD as a Classification Problem85Features for WSD
â—¼intuition: 
â‘sense of a word depends on the sense of surrounding words
â—¼ex: bass = fish, musical instrument, ...
â—¼So use a window of words around the target word as 
features
Surrounding words  Most probable sense  
â€¦riverâ€¦  fish 
â€¦violinâ€¦  instrument  
â€¦salmonâ€¦  fish 
â€¦playâ€¦ instrument  
â€¦player â€¦ instrument  
â€¦striped â€¦ fish 
 
 86Features for WSD
â—¼Take a window of nwords around the target word
â—¼Encode information about the words around the target word
â‘An electric guitar and bass player stand off to one side, 
not really part of the scene, just as a sort of nod to gringo 
expectations perhaps.87NaÃ¯ve Bayes WSD
â—¼Goal: choose the most probable sense s* for a word given a vector 
V of surrounding words
â—¼Feature vector V contains: 
â‘Features: words [ fishing, big, sound, player, fly, rod, â€¦ ]
â‘Value: frequency of these words in a window before & after the 
target word [0, 0, 0, 2, 1, 0, â€¦]
â—¼Bayes decision rule: 
â‘s* = argmaxskP(sk|V) 
â‘where:
â—¼S is the set of possible senses for the target word
â—¼skis a sense in S
â—¼V is the feature vector88â—¼Training a NaÃ¯ve Bayes classifier 
= estimating P(vj|sk) and P(sk) from a sense -tagged training 
corpus
= finding the most likely sense kNaÃ¯ve Bayes WSD
Nb of occurrences of feature j 
over the total nb of features 
appearing in windows of Sk
Nb of  occurrences of sense k 
over nb of all occurrences of 
ambiguous word
ïƒ·ïƒ·ïƒ·ïƒ·
ïƒ¸ïƒ¶
ïƒ§ïƒ§ïƒ§ïƒ§
ïƒ¨ïƒ¦
=ïƒ¥+ =n
1jkjk  
s)s|P(v log  )P(s log argmaxs*
k
  )s, count(v )s, count(v  )s|P(v
tktkj
kjïƒ¥=
 ) count(word ) count(s  )P(sk
k=89Example
â—¼Training corpus (context window = Â±3 words):
â€¦Today the World Bank/ BANK1 and partners arecalling for greater reliefâ€¦
â€¦Welcome to the Bank/ BANK1 of America the nation's leading financial institutionâ€¦ 
â€¦Welcome toAmerica's Job Bank/ BANK1 Visit our siteandâ€¦
â€¦Web site of theEuropean Central Bank/ BANK1 located in Frankfurt â€¦
â€¦TheAsian Development Bank/ BANK1 ADB a multilateral development financeâ€¦
â€¦lounging against verdant banks/ BANK2 carving out the ...
â€¦for swimming, had warned heroff the banks/ BANK2 of the Potomac . Nobody...
â—¼Training:
â‘P(the|BANK1 ) = 5/30 P(the|BANK2 ) = 3/12
â‘P(world |BANK1 ) = 1/30 P(world |BANK2 ) = 0/12
â‘P(and|BANK1 ) = 1/30 P(and|BANK2 ) = 0/12
â‘â€¦ â€¦ 
â‘P(off|BANK1 ) = 0/30 P(off|BANK2 ) = 
1/12
â‘P(Potomac |BANK1 ) = 0/30 P(Potomac |BANK2 ) = 1/12
â‘P(BANK1 ) = 5/7 P(BANK2 ) = 2/7
â—¼Disambiguation: â€œI lost my left shoe on the banks of the river Nile.â€
â‘Score( BANK1 )=log(5/7) + log(P( shoe|BANK1 ))+log(P( on|BANK1 ))+log(P( the|BANK1 )) â€¦
â‘Score( BANK2 )=log(2/7) + log(P( shoe|BANK2 )+log(P( on|BANK2 ))+log(P( the|BANK2 )) â€¦
BANK1 BANK2
90Example (with add 0.5 smoothing)
â—¼Training corpus (context window = Â±3 words):
â€¦Today the World Bank/ BANK1 and partners arecalling for greater reliefâ€¦
â€¦Welcome to the Bank/ BANK1 of America the nation's leading financial institutionâ€¦ 
â€¦Welcome toAmerica's Job Bank/ BANK1 Visit our siteandâ€¦
â€¦Web site of theEuropean Central Bank/ BANK1 located in Frankfurt â€¦
â€¦TheAsian Development Bank/ BANK1 ADB a multilateral development financeâ€¦
â€¦lounging against verdant banks/ BANK2 carving out the ...
â€¦for swimming, had warned heroff the banks/ BANK2 of the Potomac . Nobody...
â—¼Assume V = 50
â—¼Training:
â‘P(the|BANK1 ) = (5+.5) / (30+.5V) P(the|BANK2 ) = (3+.5) / (12 + .5V)
â‘P(world |BANK1 ) = (1+.5) / 55 P(world |BANK2 ) = (0+.5) / 37
â‘P(and|BANK1 ) = (1+.5) / 55 P(and|BANK2 ) = (0+.5) / 
37
â‘â€¦
â‘P(off|BANK1 ) = (0+.5) / 55 P(off|BANK2 ) = 
(1+.5) / 37
â‘P(Potomac |BANK1 ) = (0+.5) / 55 P(Potomac |BANK2 ) = (1+.5) / 
37
â‘P(BANK1 ) = 5/7
P(BANK2 ) = 2/7
â—¼Disambiguation: â€œI lost my left shoe on the banks of the river Nile.â€
â‘Score( BANK1 )=log(5/7) + log(P( shoe|BANK1 ))+log(P( on|BANK1 ))+log(P( the|BANK1 )) â€¦
â‘Score( BANK2 )=log(2/7) + log(P( shoe|BANK2 ))+log(P( on|BANK2 ))+log(P( the|BANK2 )) â€¦91Stages of NL Understanding
source: Luger (2005)Semantic interpretation: 
â—¼ Lexical Semantics : 
What is the 
meaning/semantic 
relations between 
individual words? 
Chair:  person?  Furniture?
â—¼ Compositional 
Semantics: What is the 
meaning of phrases and 
sentences?
The chairâ€™s leg is broken
92Compositional Semantics
â—¼The cat eats the mouse = The mouse is eaten by the cat.
â—¼Goal: 
â‘map an expression into a knowledge representation
â—¼a representation of context -independent, literal meaning
â—¼e.g. first -order predicate logic, conceptual graph, embedding...
â‘to assign semantic roles (different from grammatical roles):
â—¼Semantic roles: Agent, Patient, Instrument, Time,  Location, â€¦
â—¼Grammatical roles:  subject, direct object, ... 
â—¼E.g.
â‘The child hid the candy under the bed.
Hide (agent=child, patient=candy, 
location=under_the_bed, time=past)
93Some Difficulties
â—¼Syntax is not enough
â‘I ate spaghetti with a fork .  <instrument>
â‘I ate spaghetti with my sister .     <accompanying person>
â‘I ate spaghetti with meat balls .  <attribute of food>
â‘I ate spaghetti with lots of appetite . <manner>
â€¦
â‘Gun= instrument that can kill
â‘Metal gunâ€¦ a gun made out of metal    
â‘Water gunâ€¦ a gun made out of water? 
â‘Fake gunâ€¦ it is a gun anyways?  Can it kill? 
â‘General Kane â€¦ person     but  General Motors â€¦ corporation
â—¼Parallel problems to syntactic ambiguity
â‘Happy [cats and dogs] live on the farm
â‘[Happy cats] and dogs live on the farm
â—¼Quantifier Scoping
â‘Every man loves a woman.
94Stages of NLU
source: Luger (2005)â—¼ Discourse Analysis
How to relate the meaning of sentences to 
surrounding sentences?
I have to go to the store.  I need butter.
I have to go to the university.  I need butter.
â—¼ Pragmatics
How people use language in a social 
environment?
Do you have a child?    
Do you have a quarter?
â—¼ World Knowledge 
How knowledge about the world (history, 
facts, â€¦) modifies our understanding of 
text?
Bill Gates passed away last night.
95Discourse Analysis
â—¼In logics: 
â—¼Not in NL:
â‘John visited Paris.  He bought Mary some expensive 
perfume.  Then he flew home.  He went to Walmart.  He 
bought some underwear.
â‘John visited Paris. Then he flew home. He went to 
Walmart. He bought Mary some expensive perfume. He 
bought some underwear.
â—¼Humans infer relations between sentences that may not 
be explicitly stated in order to make a text coherent.
â‘(?) I am going to Concordia.  I need butter.
96Examples of Discourse Relations
CONDITION  If it rains, I will go out.
SEQUENCE Do this, then do that.
CONTRAST This is good, but this is better.
CAUSE Because I was sick, I could not do my 
assignment.
RESULT  Click on the button, the red light will 
blink.
PURPOSE To use the computer, get an access 
code.
ELABORATION The solution was developed by Alan Turing.  
Turing was a great 
mathematician living in
Great Britain. He was an 
atheist as well as gay. Another Classification Problem, again!
â—¼Discourse tagging can be viewed as typical classification problem
â—¼use machine learning techniques (ex. NaÃ¯ve Bayes classifier, decision 
tree) to train a system
â—¼that learns a classifier to assign to unseen sentences one of a fixed 
number of discourse relations (categories)
â—¼Input: 
â—¼Sentence  Ex. If it rains, I will go out.
â—¼Features? 
â–ªConnectives such as â€œ ifâ€, â€œhowever â€, â€œin conclusion â€
â–ªTense of verb (future, past)
â–ªâ€¦
â—¼Output: 
â—¼Most likely relation in the sentence (none, condition, contrast, 
purpose, â€¦)98Stages of NLU
source: Luger (2005)â—¼ Discourse Analysis
How to relate the meaning of sentences to 
surrounding sentences?
I have to go to the store.  I need butter.
I have to go to the university.  I need butter.
â—¼ Pragmatics
How people use language in a social 
environment?
Do you have a child?    
Do you have a quarter?
â—¼ World Knowledge 
How knowledge about the world (history, 
facts, â€¦) modifies our understanding of 
text?
Bill Gates passed away last night.
99Pragmatics
â—¼goes beyond the literal meaning of a sentence 
â—¼tries to explain what the speaker is really expressing
â—¼understanding how people use language socially 
â‘E.g.: figures of speech, â€¦
â‘E.g.: Could you spare some change?100Stages of NLU
source: Luger (2005)â—¼ Discourse Analysis
How to relate the meaning of sentences to 
surrounding sentences?
I have to go to the store.  I need butter.
I have to go to the university.  I need butter.
â—¼ Pragmatics
How people use language in a social 
environment?
Do you have a child?    
Do you have a quarter?
â—¼ World Knowledge 
How knowledge about the world (history, 
facts, â€¦) modifies our understanding of 
text?
Bill Gates passed away last night.
101Using World Knowledge
â—¼Using our general knowledge of the world to interpret 
a sentence/discourse
â—¼E.g.: 
The trophy would not fit in the brown suitcase because ...
... it was too big.
... itwas too small.
The professor sent the student to see the principal becauseâ€¦
â€¦hewanted to see him.
â€¦hewas throwing paper balls in class.
â€¦hecould not take it anymore.
â‘Ex: Silence of the lambsâ€¦
Current Research area: see Winograd Schema Challenge
102Summary of NLU
source: Luger (2005)Discourse Analysis
Pragmatics
World Knowledge 
Lexical Semantics
Compositional SemanticsSyntactic ParsingRecap
103Applications
linguistic features are hand -engineered and fed to the ML model  Remember these slides?
104
(to see in a few classes)
